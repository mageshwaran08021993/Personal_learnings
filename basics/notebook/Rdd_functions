['__add__',
 '__class__',
 '__delattr__',
 '__dict__',
 '__dir__',
 '__doc__',
 '__eq__',
 '__format__',
 '__ge__',
 '__getattribute__',
 '__getnewargs__',
 '__gt__',
 '__hash__',
 '__init__',
 '__init_subclass__',
 '__le__',
 '__lt__',
 '__module__',
 '__ne__',
 '__new__',
 '__reduce__',
 '__reduce_ex__',
 '__repr__',
 '__setattr__',
 '__sizeof__',
 '__str__',
 '__subclasshook__',
 '__weakref__',
 '_computeFractionForSampleSize',
 '_defaultReducePartitions',
 '_id',
 '_is_barrier',
 '_jrdd',
 '_jrdd_deserializer',
 '_memory_limit',
 '_pickled',
 '_reserialize',
 '_to_java_object_rdd',
 'aggregate',
 'aggregateByKey',
 'barrier',
 'cache', -  Stores data in memory_only and in JVM heap as unserialized objects - internally calls persist(). pyspark.StorageLevel.MEMORY_ONLY. (LRU) algorithm
 'cartesian',
 'checkpoint',
 'coalesce', - Does shuffling in the required nodes, for e.g. we have data in 4 nodes and if we do coalesce of 2 means then data from 2 nodes will be transfered.
 'cogroup',
 'collect',
 'collectAsMap',
 'collectWithJobGroup',
 'combineByKey',
 'context',
 'count',
 'countApprox',
 'countApproxDistinct',
 'countByKey',
 'countByValue',
 'ctx',
 'distinct',
 'filter', - rdd3.filter(lambda x : 'an' in x[1]) - filters the records which has 'an'
 'first', - Action - returns the first record in a rdd
 'flatMap', - it applies an functions if provided and flattens the data into single list, f=function is a mandatory
 'flatMapValues',
 'fold',
 'foldByKey',
 'foreach',
 'foreachPartition',
 'fullOuterJoin',
 'getCheckpointFile',
 'getNumPartitions', - gives the no. of partitions available in a RDD
 'getStorageLevel',
 'glom',
 'groupBy',
 'groupByKey',
 'groupWith',
 'histogram',
 'id',
 'intersection',
 'isCheckpointed',
 'isEmpty',
 'isLocallyCheckpointed',
 'is_cached',
 'is_checkpointed',
 'join',
 'keyBy',
 'keys',
 'leftOuterJoin',
 'localCheckpoint',
 'lookup',
 'map', - f=function is a mandatory, it performs the function on each record and it doesn't change the count of records.
 'mapPartitions',
 'mapPartitionsWithIndex',
 'mapPartitionsWithSplit',
 'mapValues',
 'max', -Action - returns the record with max value
 'mean',
 'meanApprox',
 'min',
 'name',
 'partitionBy',
 'partitioner',
 'persist',
 'pipe',
 'randomSplit',
 'reduce',
 'reduceByKey', - rdd3.reduceByKey(lambda a,b: a+b) will count the no. of words and sum it.
 'reduceByKeyLocally',
 'repartition', - does data full shuffle and transfers data from all nodes
 'repartitionAndSortWithinPartitions',
 'rightOuterJoin',
 'sample',
 'sampleByKey',
 'sampleStdev',
 'sampleVariance',
 'saveAsHadoopDataset',
 'saveAsHadoopFile',
 'saveAsNewAPIHadoopDataset',
 'saveAsNewAPIHadoopFile',
 'saveAsPickleFile',
 'saveAsSequenceFile',
 'saveAsTextFile',
 'setName',
 'sortBy',
 'sortByKey', - sorts the data based on Key. ascending=False means it does in Descending order.
 'stats',
 'stdev',
 'subtract',
 'subtractByKey',
 'sum',
 'sumApprox',
 'take',
 'takeOrdered',
 'takeSample',
 'toDF',
 'toDebugString',
 'toLocalIterator',
 'top',
 'treeAggregate',
 'treeReduce',
 'union',
 'unpersist',
 'values',
 'variance',
 'zip',
 'zipWithIndex',
 'zipWithUniqueId']

 Reading a file:
 sc.textFile("")
 sc.wholeTextFiles() - function returns a PairRDD with the key being the file path and value being file content
 emptyRDD - Creates an empty rdd with 0 partitions
 spark.sparkContext.parallelize([],10) - creates empty partition with 10 partition

Types of RDD's:
ShuffledRDD –
gropByKey(), reduceByKey(), join() does shuffling.
DoubleRDD –
SequenceFileRDD –
HadoopRDD –
ParallelCollectionRDD –

Shared Variables:
broadcast variables - (sc.broadcast) read-only shared variables that are cached and available on all nodes in a cluster in-order to access or use by the tasks
filteDf= df.where((df['state'].isin(broadcastStates.value)))

